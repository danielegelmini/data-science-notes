    When considering two-dimensional random variables, such as a pair of measurements like weight and height, we analyze the distribution of points in a plane. The distribution of each variable alone (marginal distribution) does not capture the relationship between the two variables. For a full understanding, we need to study the \emph{joint distribution} of the pair \((X, Y)\), which is more complex due to potential dependencies.
    
    \section{Toward Higher-Dimensional Random Variables}
    This framework sets the stage for considering pairs of random variables, or \emph{random vectors}, and expanding the analysis to multidimensional distributions. By conditioning, we can explore relationships between random variables, such as the interaction between an individual’s height and weight, or study time series data by observing values over consecutive days.
    \paragraph{Definition of a Random Vector}
    A \textbf{random vector} \( \mathbf{X} = (X_1, X_2, \ldots, X_d) \) is a random variable taking values in \( \mathbb{R}^d \), with outcomes measurable with respect to the Borel \(\sigma\)-field on \( \mathbb{R}^d \). \newline 
    The Borel \(\sigma\)-field on \( \mathbb{R}^d \), denoted \( \mathcal{B}(\mathbb{R}^d) \), is the smallest \(\sigma\)-field generated by the open sets of \( \mathbb{R}^d \). Thus, a random vector \( \mathbf{X}: \Omega \to \mathbb{R}^d \) is a function where each \( \omega \in \Omega \) maps to a tuple \( (X_1(\omega), X_2(\omega), \ldots, X_d(\omega)) \), where each \( X_i \) is a real-valued random variable.
    
    \paragraph{Distribution of a Random Vector}
    The distribution of the random vector \( \mathbf{X} \), denoted \( \mu_{\mathbf{X}} \), the law of X, is a probability measure defined on \( \mathcal{B}(\mathbb{R}^d) \) by:
    \[
    \mu_{\mathbf{X}}(B) = P(\mathbf{X} \in B),
    \]
    where \( B \in \mathcal{B}(\mathbb{R}^d) \). \newline
    This distribution, \( \mu_{\mathbf{X}} \), is called the \textbf{joint distribution} of the components \( X_1, X_2, \ldots, X_d \).
    
    \paragraph{Marginal Distributions}
    From the joint distribution \( \mu_{\mathbf{X}} \), we can obtain the marginal distributions of each individual component \( X_i \) by projecting onto each coordinate. For example, the marginal distribution of \( X_1 \) is:
    \[
    \mu_{X_1}(A) = P(X_1 \in A) = P(X_1 \in A, X_2 \in \mathbb{R}, \ldots, X_d \in \mathbb{R}),
    \]
    (commas means intersection between elements separated by the comma) which can be rewritten as:
    \[
    \mu_{X_1}(A) = \mu_{\mathbf{X}}(A \times \mathbb{R} \times \ldots \times \mathbb{R}).
    \]
    Thus, we obtain the marginal distribution for any component by integrating out the other components in \( \mu_{\mathbf{X}} \).
    
    \paragraph{Independence of Random Variables}
    The random variables \( X_1, X_2, \ldots, X_d \) are said to be \textbf{independent} if, for any choice of Borel sets \( A_1, A_2, \ldots, A_d \subseteq \mathbb{R} \),
    \[
    \mu_{\mathbf{X}}(A_1 \times A_2 \times \ldots \times A_d) = P(X_1 \in A_1, X_2 \in A_2, \ldots, X_d \in A_d) = \prod_{i=1}^d P(X_i \in A_i),
    \]
    which states that the joint probability distribution factors as the product of the marginal distributions when the components are independent.
    
    \paragraph{Uniqueness of Joint Distribution for Independent Variables}
    This independence property uniquely determines the joint distribution \( \mu_{\mathbf{X}} \). Specifically, if the components \( X_1, X_2, \ldots, X_d \) are independent, then the joint distribution \( \mu_{\mathbf{X}} \) is characterized by the product of the marginal distributions:
    \[
    \mu_{\mathbf{X}}(A_1 \times A_2 \times \ldots \times A_d) = \prod_{i=1}^d \mu_{X_i}(A_i).
    \]

\section{Discrete Random Vectors}
A \textbf{random vector} \( \mathbf{X} = (X_1, X_2, \dots, X_d) \) is \textbf{discrete} if there exists a finite or at most countable set \( N \subseteq \mathbb{R}^d \) such that:
\[
P(\mathbf{X} \in N) = 1.
\]
Thus, \( \mathbf{X} \), a \( d \)-dimensional vector, takes values in this set \( N \) with probability 1. For a discrete random vector \( \mathbf{X} = (X_1, X_2, \dots, X_d) \), we define the \textbf{joint discrete density} \( P(x_1, \dots, x_d) \) as:
\[
P(x_1, \dots, x_d) = P(X_1 = x_1, X_2 = x_2, \dots, X_d = x_d),
\]
representing the probability that each component \( X_i \) takes the value \( x_i \) for \( i = 1, \dots, d \).

\paragraph{Probability of a Discrete Random Vector in a Set}
For any measurable set \( B \) within the Borel sigma-field, the probability that the discrete random vector \( \mathbf{X} \) belongs to \( B \) is:
\[
P(\mathbf{X} \in B) = \sum_{(x_1, \dots, x_d) \in B} P(x_1, \dots, x_d).
\]
This summation extends over all tuples \( (x_1, \dots, x_d) \in B \) for which the joint discrete density is defined.

\paragraph{Marginal Densities}
Given the joint density of a discrete random vector, we can derive the \textbf{marginal densities} for each component \( X_1, X_2, \dots, X_d \). \newline
Suppose \( \mathbf{X}: \Omega \to \mathbb{R}^d \) is a discrete random vector. Then each component \( X_1, X_2, \dots, X_d \) is also discrete. \newline
For instance, the marginal density of the first component \( X_1 \) is given by:
\[
P_{X_1}(x_1) = P(X_1 = x_1) = \sum_{x_2, \dots, x_d \in \mathbb{R}} P(x_1, x_2, \dots, x_d).
\]
This summation is taken over all possible values of the other components \( x_2, x_3, \dots, x_d \), effectively “summing out” the influence of those components to obtain the marginal probability of \( X_1 \).

\paragraph{Marginalization Process}
To find the marginal density of a specific component, we:
\begin{itemize}
    \item Fix the value of the component of interest.
    \item Sum over all possible values of the remaining \( d-1 \) components in the joint density.
\end{itemize}

\paragraph{Independence of Components}
Let \( \mathbf{X} = (X_1, X_2, \dots, X_d) \) be a discrete \( d \)-dimensional random vector. The components \( X_1, X_2, \dots, X_d \) are \textbf{independent} if and only if, for any values \( x_1, x_2, \dots, x_d \in \mathbb{R} \),
\[
P(x_1, x_2, \dots, x_d) = \prod_{i=1}^d P_{X_i}(x_i),
\]
where \( P_{X_i}(x_i) \) denotes the marginal density of the \( i \)-th component.

\section{Example: Discrete Joint Density of a Random Vector}
Consider the case where we have \( d = 2 \), i.e., a two-dimensional random vector. Let \( X \) and \( Y \) denote the two components of this vector. Assume that the vector \( (X, Y) \) is discrete, and that all probability is concentrated on the following set:
\[
N = \{ (0, 1), (1, 0), (-1, 0), (0, -1) \},
\]
with each point having an equal probability of occurring. Thus, \( P((X, Y) = (i, j)) = \frac{1}{4} \) for each \( (i, j) \in N \), since \( |N| = 4 \) and the probability is uniformly distributed.

\paragraph{Joint Density}
The joint density \( P_{X,Y}(x, y) \) is defined as follows:
\[
P_{X,Y}(x, y) = \begin{cases}
\frac{1}{4} & \text{if } (x, y) \in N, \\
0 & \text{otherwise}.
\end{cases}
\]

\paragraph{Marginal Densities}
The marginal density \( P_Y(y) \) of \( Y \) is calculated by summing over all possible values of \( X \):
\[
P_Y(y) = \sum_{x \in \mathbb{R}} P_{X,Y}(x, y).
\]
For instance:
\begin{itemize}
    \item If \( y = -5 \) (or any value outside \( \{0, 1, -1\} \)), then \( P_{X,Y}(x, -5) = 0 \) for all \( x \), so \( P_Y(-5) = 0 \).
    \item Similarly, \( P_Y(y) = 0 \) for any \( y \notin \{0, 1, -1\} \).
    \item For \( y = 1 \):
  \[
  P_Y(1) = P_{X,Y}(0, 1) = \frac{1}{4}.
  \]
    \item For \( y = -1 \):
  \[
  P_Y(-1) = P_{X,Y}(0, -1) = \frac{1}{4}.
  \]
    \item For \( y = 0 \):
  \[
  P_Y(0) = P_{X,Y}(1, 0) + P_{X,Y}(-1, 0) = \frac{1}{4} + \frac{1}{4} = \frac{1}{2}.
  \]
\end{itemize}
Thus, the marginal distribution for \( Y \) is:
\[
P_Y(y) = \begin{cases}
\frac{1}{4} & \text{if } y = 1 \text{ or } y = -1, \\
\frac{1}{2} & \text{if } y = 0, \\
0 & \text{otherwise}.
\end{cases}
\]
Since the joint density \( P_{X,Y}(x, y) \) is symmetric, the marginal density \( P_X(x) \) of \( X \) will be identical:
\[
P_X(x) = \begin{cases}
\frac{1}{4} & \text{if } x = 1 \text{ or } x = -1, \\
\frac{1}{2} & \text{if } x = 0, \\
0 & \text{otherwise}.
\end{cases}
\]

\paragraph{Independence of \( X \) and \( Y \)}
To check whether \( X \) and \( Y \) are independent, we would need the joint density to factorize as:
\[
P_{X,Y}(x, y) = P_X(x) \cdot P_Y(y),
\]
for all \( (x, y) \in \mathbb{R}^2 \). \newline
For example, consider \( (x, y) = (0, 0) \):
\[
P_{X,Y}(0, 0) = 0, \quad P_X(0) \cdot P_Y(0) = \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4}.
\]
Since \( P_{X,Y}(0, 0) \neq P_X(0) \cdot P_Y(0) \), the random variables \( X \) and \( Y \) are not independent. \newline
In this distribution, knowing the value of \( X \) changes the distribution of \( Y \). For example, if \( X = 0 \), then \( Y \) is constrained to take values in \( \{1, -1\} \), rather than being spread across all values. Thus, \( X \) and \( Y \) are dependent.

\paragraph{Expectation of the Product of Independent Variables}
Let \(X\) and \(Y\) be random variables with the joint density concentrated on the set \( \{ (0,1), (1,0), (0,-1), (-1,0) \} \) with equal probability \( \frac{1}{4} \). \newline
To find \(E[XY]\), we use the expectation formula for discrete variables:
\[
E[XY] = \sum_{(x,y) \in \{ (0,1), (1,0), (0,-1), (-1,0) \}} x \cdot y \cdot p(x, y).
\]
Since \(XY = 0\) for each point in this set, we have:
\[
E[XY] = 0.
\]

\section{Absolutely Continuous Random Vector}
A random vector \( X \), defined as a function \( X: \Omega \rightarrow \mathbb{R}^d \), is said to be \textit{absolutely continuous} if there exists a function \( f_X \) on \( \mathbb{R}^d \) with values in \( [0, +\infty) \) such that, for any Borel set \( B \subset \mathbb{R}^d \),
\[
P(X \in B) = \int_B f_X(t_1, \dots, t_d) \, dt_1 \dots dt_d.
\]
Here, \( f_X \) is known as the \textit{joint density} of \( X = (X_1, \dots, X_d) \), where each component \( X_i \) of \( X \) is also absolutely continuous.

\paragraph{Marginal Density of a Component}
For each component \( X_i \), the marginal density \( f_{X_i} \) can be obtained by integrating the joint density \( f_X \) over the other \( d-1 \) components:
\[
f_{X_i}(x_i) = \int_{\mathbb{R}^{d-1}} f_X(t_1, \dots, t_{i-1}, x_i, t_{i+1}, \dots, t_d) \, dt_1 \dots dt_{i-1} dt_{i+1} \dots dt_d.
\]

\paragraph{Independence of Absolutely Continuous Random Variables}
\textbf{Proposition.} Let \( X_1, X_2, \dots, X_d \) be absolutely continuous random variables. They are independent if and only if the joint density \( f_X \) can be factorized as:
\[
f_X(x_1, x_2, \dots, x_d) = \prod_{i=1}^d f_{X_i}(x_i),
\]
where this equality holds almost everywhere, i.e., outside a set of measure zero in \( \mathbb{R}^d \).

\paragraph{Definition: Independence of Random Variables}
Two random variables \(X\) and \(Y\) are \textit{independent} if, and only if, their joint density \( f_{X,Y}(x,y) \) can be written as the product of their marginal densities:
\[
f_{X,Y}(x, y) = f_X(x) f_Y(y),
\]
where
\[
f_X(x) = \int_{\mathbb{R}} f_{X,Y}(x, t) \, dt \quad \text{and} \quad f_Y(y) = \int_{\mathbb{R}} f_{X,Y}(s, y) \, ds.
\]
In other words, by integrating over the other variable, we obtain the marginal density of \(X\) and \(Y\) respectively.

\paragraph{Expectation of Functions of Random Vectors}
Let \(X = (X_1, X_2, \dots, X_d)\) be a \(d\)-dimensional random vector and \(g : \mathbb{R}^d \to \mathbb{R}\) a measurable function. The expectation of \(g(X)\) is given by:
\begin{itemize}
    \item If \(X\) is discrete:
    \[
    E[g(X)] = \sum_{x \in \mathbb{R}^d} g(x) p_X(x),
    \]
    where \(p_X(x)\) is the joint probability mass function of \(X\).
    \item If \(X\) is absolutely continuous:
    \[
    E[g(X)] = \int_{\mathbb{R}^d} g(x) f_X(x) \, dx,
    \]
    where \(f_X(x)\) is the joint density of \(X\).
\end{itemize}

\section{Example: Density Over a Circular Region}
Consider the following example where we define a density over a two-dimensional region. \newline
Let \( f_{X,Y} \) be defined as follows:
\[
f_{X,Y}(x, y) = \begin{cases}
c & \text{if } x^2 + y^2 \leq 1, \\
0 & \text{otherwise}.
\end{cases}
\]
To determine the value of \( c \), we use the condition that the integral of the density over the entire space \( \mathbb{R}^2 \) must equal 1:
\[
\int_{\mathbb{R}^2} f_{X,Y}(x, y) \, dx \, dy = 1.
\]
Since \( f_{X,Y} \) is zero outside the unit disk, we have:
\[
\int_{D} c \, dx \, dy = 1,
\]
where \( D = \{ (x, y) : x^2 + y^2 \leq 1 \} \). The area of \( D \) is \( \pi \), so:
\[
c \cdot \pi = 1 \implies c = \frac{1}{\pi}.
\]
Thus, the density \( f_{X,Y} \) is:
\[
f_{X,Y}(x, y) = \begin{cases}
\frac{1}{\pi} & \text{if } x^2 + y^2 \leq 1, \\
0 & \text{otherwise}.
\end{cases}
\]
For two random variables \( X \) and \( Y \) to be independent, the joint density must factorize into the product of the marginal densities. In this case, since the density is defined over a circular region, it cannot be written as a Cartesian product of intervals. Therefore, \( X \) and \( Y \) are not independent. \newline
To find the marginal density of \(X\), we integrate over \(Y\):
\[
f_X(x) = \int_{-\sqrt{1 - x^2}}^{\sqrt{1 - x^2}} \frac{1}{\pi} \, dy = \frac{1}{\pi} \cdot 2 \sqrt{1 - x^2} = \frac{2 \sqrt{1 - x^2}}{\pi},
\]
valid for \( -1 \leq x \leq 1 \). \newline
Similarly, for \(Y\), we have
\[
f_Y(y) = \frac{2 \sqrt{1 - y^2}}{\pi} \quad \text{for } -1 \leq y \leq 1.
\]






