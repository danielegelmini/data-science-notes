\chapter{Asymptotical Analysis}
    \section{Introduction to Asymptotic Analysis}
    
    Asymptotic analysis is used to describe the running time of an algorithm in terms of the input size \(n\) as \(n \rightarrow \infty\). It allows us to abstract away constants and focus on the growth of functions, comparing algorithms based on their efficiency. \newline    
    Let us consider two algorithms, \(A\) and \(B\), where the running time of algorithm \(A\), denoted as \(T_A(n) \sim n^2\), while for algorithm \(B\), the running time \(T_B(n) \sim n\). We can conclude that algorithm \(B\) is more efficient than algorithm \(A\) for all \(n \geq n_0\), where \(n_0\) is some sufficiently large threshold.
    
    \section{Asymptotic Notation}
    
    We use asymptotic notation to formalize this comparison. The goal is to describe the behavior of functions \(f(n)\) and \(g(n)\) as \(n\) becomes large. Consider the following types of asymptotic notation:
    
    \subsection{Big-Theta (\(\Theta\)) Notation}
    
    The function \(T_A(n)\) is said to be \(\Theta(g(n))\), denoted as:
    \[
    T_A(n) = \Theta(n^2)
    \]
    This means that \(T_A(n)\) is asymptotically bounded both above and below by \(g(n)\). Formally, 
    \[
    f(n) = \Theta(g(n))
    \] if there exist constants \(c_1, c_2 > 0\) and \(n_0 > 0\) such that:
    \[
    0 \leq c_1 g(n) \leq f(n) \leq c_2 g(n) \quad \text{for all} \quad n \geq n_0.
    \]
    In other words, \(f(n)\) lies between \(c_1 g(n)\) and \(c_2 g(n)\) for sufficiently large \(n\).
    
    \subsubsection{Example:}
    Let’s prove that \(f(n) = \frac{1}{2}n^2 - 3n\) is \(\Theta(n^2)\). \newline
    We need to find constants \(c_1, c_2 > 0\) and \(n_0 > 0\) such that:
    \[
    c_1 n^2 \leq \frac{1}{2}n^2 - 3n \leq c_2 n^2 \quad \text{for all} \quad n \geq n_0.
    \]
    1. Divide both sides by \(n^2\):
    \[
    c_1 \leq \frac{1}{2} - \frac{3}{n} \leq c_2.
    \]
    2. Choose \(n_0 = 7\), \(c_1 = \frac{1}{14}\), and \(c_2 = \frac{1}{2}\), ensuring that the inequality holds for all \(n \geq n_0\).
    
    \subsection{Big-O (\(O\)) Notation}
    
    Big-O notation defines an upper bound on the growth of a function, so provides an upper bound on the time taken by an algorithm in terms of the size of the input. For example, if \(T_B(n) = O(n)\), it means that \(T_B(n)\) grows at most as fast as \(n\). Formally, \(f(n) = O(g(n))\) if there exists a constant \(c > 0\) and \(n_0 > 0\) such that:
    \[
    0 \leq f(n) \leq c g(n) \quad \text{for all} \quad n \geq n_0.
    \] $\\$
    The \(f(n)\) function represents the number of operations (steps) that an algorithm performs to solve a problem of size n.
    
    \subsubsection{Example:}
    Let’s prove that \(f(n) = n + 4\) is \(O(n^2)\). \newline
    1. We need to find constants \(c > 0\) and \(n_0 > 0\) such that:
    \[
    f(n) \leq c n^2 \quad \text{for all} \quad n \geq n_0.
    \]
    2. We can choose \(c = 1\) and \(n_0 = 1\) to satisfy the inequality \(n + 4 \leq n^2\) for all \(n \geq n_0\).
    
    \subsection{Big-Omega (\(\Omega\)) Notation}
    
    Big-Omega notation provides a lower bound for the growth of a function. For example, \(T_A(n) = \Omega(n^2)\) means that \(T_A(n)\) grows at least as fast as \(n^2\). Formally, \(f(n) = \Omega(g(n))\) if there exists a constant \(c > 0\) and \(n_0 > 0\) such that:
    \[
    f(n) \geq c g(n) \quad \text{for all} \quad n \geq n_0.
    \]
    
    \subsubsection{Example:}
    Let’s prove that \(6n^3\) is not \(\Theta(n^2)\) by contradiction.
    \newline
    Assume that \(6n^3 = O(n^2)\), meaning there exists a constant \(c > 0\) such that:
    \[
    6n^3 \leq c n^2 \quad \text{for all} \quad n \geq n_0.
    \]
    Dividing both sides by \(n^2\), we get:
    \[
    6n \leq c.
    \]
    This leads to a contradiction because \(6n\) grows unbounded as \(n\) increases, while \(c\) is a constant.
    
    \section{General Polynomial Functions and Their Growth}
    
    For any polynomial function of degree \(d\), such as:
    \[
    f(n) = a_d n^d + a_{d-1} n^{d-1} + \cdots + a_1 n + a_0,
    \]
    we can say that \(f(n) = \Theta(n^d)\). The lower-order terms and constants do not affect the asymptotic growth, so:
    \[
    f(n) = \Theta(n^d).
    \]
    
    \section{Theorem: Relationship Between Asymptotic Notations}
    
    For any two functions \(f(n)\) and \(g(n)\):
    \[
    f(n) = \Theta(g(n)) \quad \text{if and only if} \quad f(n) = O(g(n)) \quad \text{and} \quad f(n) = \Omega(g(n)).
    \]
    This theorem implies that if \(f(n)\) is both \(O(g(n))\) and \(\Omega(g(n))\), it must also be \(\Theta(g(n))\).
    
    \subsubsection{Example:}
    Let’s prove that \(f(n) = (n + a)^b\) is \(O(n^b)\) for \(b \geq 0\). Divide both sides by \(n^b\), then simplify:
    \[
    \frac{(n+a)^b}{n^b} = \left( 1 + \frac{a}{n} \right)^b \leq 2,
    \]
    showing that \(f(n)\) is \(O(n^b)\).
    
    \subsection{Exponential Growth}
    
    Finally, consider an exponential function like \(f(n) = 2^{n+1}\). We want to show that \(f(n) = O(2^n)\). \newline 
    1. Start by simplifying \(2^{n+1}\):
    \[
    2^{n+1} = 2 \cdot 2^n.
    \]
    2. This shows that \(f(n) = O(2^n)\), where the constant \(c = 2\).
